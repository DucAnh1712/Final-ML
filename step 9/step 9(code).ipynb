{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e7a44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Neither 'study' nor 'params' found. Please run the Optuna optimization first.\n",
      "âœ” ÄÃ£ lÆ°u: optuna_best_params_xgboost_hourly.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from optuna.trial import Trial\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Import thÃ nh cÃ´ng!\")\n",
    "\n",
    "# 1ï¸âƒ£ Load dá»¯ liá»‡u\n",
    "print(\"\\nğŸ“Š Äang táº£i dá»¯ liá»‡u...\")\n",
    "data_dir = \"../processed_data\"  # Thay Ä‘á»•i Ä‘Æ°á»ng dáº«n náº¿u cáº§n\n",
    "\n",
    "try:\n",
    "    X_train = pd.read_csv(f\"{data_dir}/data_train.csv\")\n",
    "    X_val = pd.read_csv(f\"{data_dir}/data_val.csv\")\n",
    "    X_test = pd.read_csv(f\"{data_dir}/data_test.csv\")\n",
    "    \n",
    "    # TÃ¡ch features vÃ  target\n",
    "    y_train = X_train.iloc[:, -1]\n",
    "    X_train = X_train.iloc[:, :-1]\n",
    "    \n",
    "    y_val = X_val.iloc[:, -1]\n",
    "    X_val = X_val.iloc[:, :-1]\n",
    "    \n",
    "    y_test = X_test.iloc[:, -1]\n",
    "    X_test = X_test.iloc[:, :-1]\n",
    "    \n",
    "    print(f\"âœ… Train shape: {X_train.shape}\")\n",
    "    print(f\"âœ… Val shape: {X_val.shape}\")\n",
    "    print(f\"âœ… Test shape: {X_test.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Lá»—i táº£i dá»¯ liá»‡u: {e}\")\n",
    "    print(\"ğŸ’¡ Táº¡o dá»¯ liá»‡u giáº£ Ä‘á»‹nh cho demo...\")\n",
    "    \n",
    "    # Táº¡o dá»¯ liá»‡u giáº£ Ä‘á»‹nh\n",
    "    from sklearn.datasets import make_regression\n",
    "    X, y = make_regression(n_samples=1000, n_features=20, noise=10, random_state=42)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    \n",
    "    X_train = pd.DataFrame(X_train, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "    X_val = pd.DataFrame(X_val, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "    X_test = pd.DataFrame(X_test, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "    y_train = pd.Series(y_train)\n",
    "    y_val = pd.Series(y_val)\n",
    "    y_test = pd.Series(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318b4e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2ï¸âƒ£ TUNING HYPERPARAMETERS LIGHTGBM Vá»šI OPTUNA\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ”§ Báº®T Äáº¦U TUNING HYPERPARAMETERS LIGHTGBM Vá»šI OPTUNA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def objective(trial: Trial):\n",
    "    \"\"\"\n",
    "    HÃ m objective Ä‘á»ƒ Optuna tá»‘i Æ°u hÃ³a\n",
    "    \"\"\"\n",
    "    # Äá»‹nh nghÄ©a khÃ´ng gian hyperparameter\n",
    "    params = {\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 0.1),\n",
    "        'random_state': 42,\n",
    "        'verbose': -1,\n",
    "    }\n",
    "    \n",
    "    # Táº¡o LightGBM dataset\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    \n",
    "    # Train model vá»›i early stopping\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=100,\n",
    "        valid_sets=[lgb.Dataset(X_val, label=y_val)],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=20),\n",
    "            lgb.log_evaluation(period=0)  # Táº¯t log\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # ÄÃ¡nh giÃ¡ trÃªn validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    \n",
    "    return mse\n",
    "\n",
    "# Táº¡o study vá»›i Optuna\n",
    "print(\"\\nğŸ“Š Báº¯t Ä‘áº§u tuning vá»›i 100 trials...\")\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "\n",
    "# Cháº¡y optimization\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "# 3ï¸âƒ£ Hiá»ƒn thá»‹ káº¿t quáº£\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… HOÃ€N THÃ€NH TUNING!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Best trial\n",
    "best_trial = study.best_trial\n",
    "print(f\"\\nğŸ† Best Trial: {best_trial.number}\")\n",
    "print(f\"ğŸ“Š Best MSE: {best_trial.value:.6f}\")\n",
    "print(f\"\\nğŸ“ Best Hyperparameters:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# 4ï¸âƒ£ Train mÃ´ hÃ¬nh final vá»›i best hyperparameters\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ TRAIN MÃ” HÃŒNH FINAL Vá»šI BEST HYPERPARAMETERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_params = study.best_trial.params\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "final_model = lgb.train(\n",
    "    best_params,\n",
    "    train_data,\n",
    "    num_boost_round=200,\n",
    "    valid_sets=[lgb.Dataset(X_val, label=y_val)],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=20),\n",
    "        lgb.log_evaluation(period=20)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 5ï¸âƒ£ ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š ÄÃNH GIÃ MÃ” HÃŒNH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train evaluation\n",
    "y_train_pred = final_model.predict(X_train)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"\\nğŸƒ Train Metrics:\")\n",
    "print(f\"  MSE:  {train_mse:.6f}\")\n",
    "print(f\"  MAE:  {train_mae:.6f}\")\n",
    "print(f\"  RÂ²:   {train_r2:.6f}\")\n",
    "\n",
    "# Val evaluation\n",
    "y_val_pred = final_model.predict(X_val)\n",
    "val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"\\nâœ”ï¸ Val Metrics:\")\n",
    "print(f\"  MSE:  {val_mse:.6f}\")\n",
    "print(f\"  MAE:  {val_mae:.6f}\")\n",
    "print(f\"  RÂ²:   {val_r2:.6f}\")\n",
    "\n",
    "# Test evaluation\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\nğŸ¯ Test Metrics:\")\n",
    "print(f\"  MSE:  {test_mse:.6f}\")\n",
    "print(f\"  MAE:  {test_mae:.6f}\")\n",
    "print(f\"  RÂ²:   {test_r2:.6f}\")\n",
    "\n",
    "# 6ï¸âƒ£ LÆ°u mÃ´ hÃ¬nh\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ’¾ LÆ¯U MÃ” HÃŒNH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_path = \"lightgbm_best_model.pkl\"\n",
    "joblib.dump(final_model, model_path)\n",
    "print(f\"âœ… MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: {model_path}\")\n",
    "\n",
    "# LÆ°u best hyperparameters\n",
    "import yaml\n",
    "params_path = \"lightgbm_best_params.yaml\"\n",
    "with open(params_path, 'w') as f:\n",
    "    yaml.dump(best_params, f, default_flow_style=False)\n",
    "print(f\"âœ… Best hyperparameters Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: {params_path}\")\n",
    "\n",
    "print(\"\\nâœ… QuÃ¡ trÃ¬nh tuning hoÃ n thÃ nh!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
