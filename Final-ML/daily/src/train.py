# train.py
import os
import pandas as pd
import numpy as np
import joblib
import yaml
import optuna
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error
from xgboost import XGBRegressor
from clearml import Task # <-- Báº¡n Ä‘ang dÃ¹ng ClearML, giá»¯ láº¡i

# Import from other files
import config
from feature_engineering import create_feature_pipeline

# ======================================================
# === HÃ€M Má»šI: Táº¢I Dá»® LIá»†U CHO NHIá»€U TARGET ===
# ======================================================
def load_features_for_tuning_multi(target_cols_list):
    """
    Táº£i features (X) tá»« feature_data/
    Táº£i Táº¤T Cáº¢ cÃ¡c targets (y) tá»« processed_data/
    """
    print("ðŸ” Loading aligned data for tuning (X from features, Y-dict from processed)...")
    
    # 1. Táº£i FEATURES (X) (ÄÃ£ Ä‘Æ°á»£c táº¡o vÃ  dropna)
    train_feat_X_path = os.path.join(config.FEATURE_DIR, "feature_train.csv")
    val_feat_X_path = os.path.join(config.FEATURE_DIR, "feature_val.csv")
    
    if not os.path.exists(train_feat_X_path) or not os.path.exists(val_feat_X_path):
        raise FileNotFoundError(
            "Feature files not found. Please run feature_engineering.py first."
        )
        
    train_feat_X = pd.read_csv(train_feat_X_path)
    val_feat_X = pd.read_csv(val_feat_X_path)
    X_tune = pd.concat([train_feat_X, val_feat_X], ignore_index=True)

    # 2. Táº£i dá»¯ liá»‡u PROCESSED (Äá»ƒ láº¥y Táº¤T Cáº¢ CÃC Cá»˜T Y)
    train_proc_path = os.path.join(config.PROCESSED_DATA_DIR, "data_train.csv")
    val_proc_path = os.path.join(config.PROCESSED_DATA_DIR, "data_val.csv")
    
    if not os.path.exists(train_proc_path) or not os.path.exists(val_proc_path):
        raise FileNotFoundError(
            "Processed data files not found. Please run data_processing.py first."
        )

    train_proc = pd.read_csv(train_proc_path)
    val_proc = pd.read_csv(val_proc_path)
    
    # 3. CÄ‚N CHá»ˆNH (ALIGN) y Vá»šI X (CÄƒn chá»‰nh cÃ¡c hÃ ng bá»‹ drop á»Ÿ Äáº¦U)
    original_train_len = len(train_proc)
    new_train_len = len(train_feat_X)
    rows_dropped_at_start = original_train_len - new_train_len
    
    if rows_dropped_at_start < 0:
        raise ValueError("Feature train set is larger than processed train set. Check logic.")
        
    print(f"Aligning data: {rows_dropped_at_start} rows were dropped from train set by feature_engineering (due to rolling windows).")

    # Táº¡o má»™t dictionary (tá»« Ä‘iá»ƒn) cho cÃ¡c Y
    y_tune_dict = {}
    
    for target_name in target_cols_list:
        # Láº¥y y (target) tá»« cÃ¡c file processed, Bá»Ž ÄI cÃ¡c hÃ ng Ä‘áº§u tiÃªn
        y_train = train_proc[target_name].iloc[rows_dropped_at_start:]
        y_val = val_proc[target_name] # Táº­p val khÃ´ng bá»‹ dropna

        y_tune = pd.concat([y_train, y_val], ignore_index=True)
        
        # LÆ°u y (váº«n cÃ²n NaN á»Ÿ cuá»‘i) vÃ o dictionary
        y_tune_dict[target_name] = y_tune

    # 4. Kiá»ƒm tra
    if len(X_tune) != len(y_tune_dict[target_cols_list[0]]):
        raise ValueError("Data misalignment after start alignment. Check logic.")
        
    obj_cols = X_tune.select_dtypes(include=['object']).columns
    if not obj_cols.empty:
        print(f"âš ï¸ Dropping object columns from X_tune: {list(obj_cols)}")
        X_tune = X_tune.drop(columns=obj_cols)

    # Tráº£ vá» X (Ä‘Ã£ cÄƒn chá»‰nh start) vÃ  Dict Y (Ä‘Ã£ cÄƒn chá»‰nh start, cÃ²n NaN á»Ÿ end)
    return X_tune, y_tune_dict
# ======================================================

def xgb_objective(trial, X, y):
    """Objective function for Optuna (Báº£n Ä‘Æ¡n giáº£n, khÃ´ng Pruning)."""
    tscv = TimeSeriesSplit(n_splits=config.CV_SPLITS)
    rmse_scores = []

    params = {
        'n_estimators': trial.suggest_int("n_estimators", 100, 1000),
        'max_depth': trial.suggest_int("max_depth", 3, 10),
        'learning_rate': trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
        'subsample': trial.suggest_float("subsample", 0.6, 1.0),
        'colsample_bytree': trial.suggest_float("colsample_bytree", 0.6, 1.0),
        'gamma': trial.suggest_float("gamma", 0.0, 5.0),
        'min_child_weight': trial.suggest_int("min_child_weight", 1, 10),
        'random_state': 42,
        'n_jobs': -1
    }

    for train_idx, val_idx in tscv.split(X):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

        pipe = Pipeline([
            ("scaler", RobustScaler()),
            ("xgb", XGBRegressor(**params))
        ])

        pipe.fit(X_train, y_train)
        preds = pipe.predict(X_val)
        rmse_scores.append(np.sqrt(mean_squared_error(y_val, preds)))

    return np.mean(rmse_scores)

# ======================================================
# === HÃ€M MAIN ÄÃƒ ÄÆ¯á»¢C VIáº¾T Láº I HOÃ€N TOÃ€N ===
# ======================================================
def main():
    """Main pipeline: Cháº¡y 4 láº§n, 1 láº§n cho má»—i target."""
    
    # 1. Initialize ClearML (Step 5)
    task = Task.init(
        project_name=config.CLEARML_PROJECT_NAME,
        task_name=config.CLEARML_TASK_NAME,
        tags=["Optuna", "XGBoost", "Multi-Target", "RollingOnly"]
    )
    
    # 1. Táº£i dá»¯ liá»‡u (cho Optuna)
    # X_tune: DataFrame (features)
    # y_tune_dict: Dictionary {"target_T1": Series, "target_T3": Series, ...}
    X_tune_full, y_tune_dict_full = load_features_for_tuning_multi(
        config.TARGET_FORECAST_COLS
    )

    # Dictionary Ä‘á»ƒ lÆ°u cÃ¡c params tá»‘t nháº¥t
    all_best_params = {}

    # === Bá»ŒC TRONG VÃ’NG Láº¶P ===
    for target_name in config.TARGET_FORECAST_COLS:
        print(f"\nðŸš€ðŸš€ðŸš€ Báº¯t Ä‘áº§u quy trÃ¬nh cho: {target_name} ðŸš€ðŸš€ðŸš€")
        
        y_tune = y_tune_dict_full[target_name]
        
        # === CÄ‚N CHá»ˆNH (ALIGN) END ===
        # Quan trá»ng: XÃ³a cÃ¡c hÃ ng NaN á»Ÿ cuá»‘i (do shift) Cá»¦A TARGET NÃ€Y
        valid_indices_tune = y_tune.dropna().index
        X_tune_aligned = X_tune_full.loc[valid_indices_tune]
        y_tune_aligned = y_tune.loc[valid_indices_tune]
        
        print(f"Aligning data for {target_name}: Dropped {len(y_tune) - len(valid_indices_tune)} NaN rows from end.")
        
        # 3. Run Optuna (Cho target nÃ y)
        print(f"ðŸš€ Starting Optuna tuning for {target_name}...")
        study = optuna.create_study(direction="minimize")
        study.optimize(
            lambda trial: xgb_objective(trial, X_tune_aligned, y_tune_aligned), 
            n_trials=config.OPTUNA_TRIALS
        )
        
        best_params = study.best_params
        all_best_params[target_name] = best_params
        print(f"ðŸ† Best Params found for {target_name}: {best_params}")
        
        # Log to ClearML
        task.connect(best_params, name=f'Best Hyperparameters ({target_name})')
        task.get_logger().report_scalar(f"best_rmse ({target_name})", "RMSE", value=study.best_value, iteration=0)

        # 4. Táº¡o Production Pipeline (Cho target nÃ y)
        print(f"ðŸ› ï¸ Creating final production pipeline for {target_name}...")
        production_pipeline = Pipeline([
            ('feature_engineering', create_feature_pipeline()),
            ('scaler', RobustScaler()),
            ('model', XGBRegressor(**best_params, random_state=42, n_jobs=-1))
        ])

        # 5. Retrain (Cho target nÃ y)
        print(f"ðŸ”„ Retraining pipeline on (Train + Val) for {target_name}...")
        train_df = pd.read_csv(os.path.join(config.PROCESSED_DATA_DIR, "data_train.csv"))
        val_df = pd.read_csv(os.path.join(config.PROCESSED_DATA_DIR, "data_val.csv"))
        all_train_data = pd.concat([train_df, val_df], ignore_index=True)
        all_train_data = all_train_data.sort_values("datetime").reset_index(drop=True)
        
        # TÃ¡ch X vÃ  y (dÃ¹ng Ä‘Ãºng target_name)
        y_train_full = all_train_data[target_name]
        
        # X_train_full lÃ  Táº¤T Cáº¢, nhÆ°ng pháº£i drop cÃ¡c cá»™t target khÃ¡c
        # vÃ  cá»™t 'temp' gá»‘c
        cols_to_drop_prod = config.TARGET_FORECAST_COLS + [config.TARGET_COL]
        X_train_full = all_train_data.drop(columns=cols_to_drop_prod, errors='ignore')
        
        # CÄƒn chá»‰nh (Align) END cho Production
        valid_indices_prod = y_train_full.dropna().index
        X_train_full_aligned = X_train_full.loc[valid_indices_prod]
        y_train_full_aligned = y_train_full.loc[valid_indices_prod]

        production_pipeline.fit(X_train_full_aligned, y_train_full_aligned)

        # 6. LÆ°u Model (Cho target nÃ y)
        model_name = f"{target_name}_pipeline.pkl"
        model_path = os.path.join(config.MODEL_DIR, model_name)
        joblib.dump(production_pipeline, model_path)
        print(f"âœ… Production pipeline saved to: {model_path}")
    
        # ======================================================
        # 7. SAVE TO ONNX FORMAT (STEP 9) (Cho target nÃ y)
        # ======================================================
        print(f"ðŸ› ï¸ Creating ONNX components for {target_name}...")
        
        scaler = RobustScaler() 
        scaler.fit(X_tune_aligned) # DÃ¹ng X_tune Ä‘Ã£ cÄƒn chá»‰nh cho target nÃ y
        
        X_train_scaled = scaler.transform(X_tune_aligned)

        model_xgb = XGBRegressor(**best_params, random_state=42, n_jobs=-1)
        model_xgb.fit(X_train_scaled, y_tune_aligned)

        # 4. LÆ¯U 2 FILE RIÃŠNG BIá»†T (vá»›i tÃªn target)
        scaler_name = f"scaler_{target_name}.pkl"
        model_json_name = f"model_{target_name}.json"
        
        scaler_path = os.path.join(config.MODEL_DIR, scaler_name)
        joblib.dump(scaler, scaler_path)
        print(f"âœ… ONNX Scaler saved to: {scaler_path}")
        
        model_json_path = os.path.join(config.MODEL_DIR, model_json_name)
        model_xgb.save_model(model_json_path)
        print(f"âœ… ONNX XGBoost Model saved to: {model_json_path}")
        # ======================================================

    # Save best_params.yaml
    params_path = os.path.join(config.MODEL_DIR, "all_best_params.yaml")
    with open(params_path, "w") as f:
        yaml.dump(all_best_params, f)
    
    task.close()
    print("\nðŸŽ‰ðŸŽ‰ðŸŽ‰ Completed training. ðŸŽ‰ðŸŽ‰ðŸŽ‰")

if __name__ == "__main__":
    main()