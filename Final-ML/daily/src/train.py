# train.py
import os
import pandas as pd
import numpy as np
import joblib
import yaml
import optuna
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.pipeline import Pipeline
# === TH√äM METRICS ƒê·ªÇ CHECK OVERFITTING ===
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from xgboost import XGBRegressor
from clearml import Task 

# Import from other files
import config
from feature_engineering import create_feature_pipeline

# ======================================================
# === H√ÄM M·ªöI: T·∫¢I D·ªÆ LI·ªÜU CHO NHI·ªÄU TARGET ===
# ======================================================
def load_features_for_tuning_multi(target_cols_list):
    """
    T·∫£i features (X) t·ª´ feature_data/
    T·∫£i T·∫§T C·∫¢ c√°c targets (y) t·ª´ processed_data/
    """
    print("üîç Loading aligned data for tuning (X from features, Y-dict from processed)...")
    
    # 1. T·∫£i FEATURES (X) (ƒê√£ ƒë∆∞·ª£c t·∫°o v√† dropna)
    train_feat_X_path = os.path.join(config.FEATURE_DIR, "feature_train.csv")
    val_feat_X_path = os.path.join(config.FEATURE_DIR, "feature_val.csv")
    
    if not os.path.exists(train_feat_X_path) or not os.path.exists(val_feat_X_path):
        raise FileNotFoundError(
            "Feature files not found. Please run feature_engineering.py first."
        )
        
    train_feat_X = pd.read_csv(train_feat_X_path)
    val_feat_X = pd.read_csv(val_feat_X_path)
    X_tune = pd.concat([train_feat_X, val_feat_X], ignore_index=True)

    # 2. T·∫£i d·ªØ li·ªáu PROCESSED (ƒê·ªÉ l·∫•y T·∫§T C·∫¢ C√ÅC C·ªòT Y)
    train_proc_path = os.path.join(config.PROCESSED_DATA_DIR, "data_train.csv")
    val_proc_path = os.path.join(config.PROCESSED_DATA_DIR, "data_val.csv")
    
    if not os.path.exists(train_proc_path) or not os.path.exists(val_proc_path):
        raise FileNotFoundError(
            "Processed data files not found. Please run data_processing.py first."
        )

    train_proc = pd.read_csv(train_proc_path)
    val_proc = pd.read_csv(val_proc_path)
    
    # 3. CƒÇN CH·ªàNH (ALIGN) y V·ªöI X (CƒÉn ch·ªânh c√°c h√†ng b·ªã drop ·ªü ƒê·∫¶U)
    original_train_len = len(train_proc)
    new_train_len = len(train_feat_X)
    rows_dropped_at_start = original_train_len - new_train_len
    
    if rows_dropped_at_start < 0:
        raise ValueError("Feature train set is larger than processed train set. Check logic.")
        
    print(f"Aligning data: {rows_dropped_at_start} rows were dropped from train set by feature_engineering (due to rolling windows).")

    # T·∫°o m·ªôt dictionary (t·ª´ ƒëi·ªÉn) cho c√°c Y
    y_tune_dict = {}
    
    for target_name in target_cols_list:
        # L·∫•y y (target) t·ª´ c√°c file processed, B·ªé ƒêI c√°c h√†ng ƒë·∫ßu ti√™n
        y_train = train_proc[target_name].iloc[rows_dropped_at_start:]
        y_val = val_proc[target_name] # T·∫≠p val kh√¥ng b·ªã dropna

        y_tune = pd.concat([y_train, y_val], ignore_index=True)
        
        # L∆∞u y (v·∫´n c√≤n NaN ·ªü cu·ªëi) v√†o dictionary
        y_tune_dict[target_name] = y_tune

    # 4. Ki·ªÉm tra
    if len(X_tune) != len(y_tune_dict[target_cols_list[0]]):
        raise ValueError("Data misalignment after start alignment. Check logic.")
        
    obj_cols = X_tune.select_dtypes(include=['object']).columns
    if not obj_cols.empty:
        print(f"‚ö†Ô∏è Dropping object columns from X_tune: {list(obj_cols)}")
        X_tune = X_tune.drop(columns=obj_cols)

    # Tr·∫£ v·ªÅ X (ƒë√£ cƒÉn ch·ªânh start) v√† Dict Y (ƒë√£ cƒÉn ch·ªânh start, c√≤n NaN ·ªü end)
    return X_tune, y_tune_dict
# ======================================================

def xgb_objective(trial, X, y):
    """Objective function for Optuna (B·∫£n ƒë∆°n gi·∫£n, kh√¥ng Pruning)."""
    tscv = TimeSeriesSplit(n_splits=config.CV_SPLITS)
    rmse_scores = []

    params = {
        'n_estimators': trial.suggest_int("n_estimators", 100, 1000),
        'max_depth': trial.suggest_int("max_depth", 2, 6),
        'learning_rate': trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
        'subsample': trial.suggest_float("subsample", 0.6, 1.0),
        'colsample_bytree': trial.suggest_float("colsample_bytree", 0.6, 1.0),
        'gamma': trial.suggest_float("gamma", 0.0, 5.0),
        'min_child_weight': trial.suggest_int("min_child_weight", 5, 20),
        'reg_alpha': trial.suggest_float("reg_alpha", 0.0, 5.0),
        'reg_lambda': trial.suggest_float("reg_lambda", 0.0, 5.0),
        'random_state': 42,
        'n_jobs': -1
    }

    for train_idx, val_idx in tscv.split(X):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

        pipe = Pipeline([
            ("scaler", RobustScaler()),
            ("xgb", XGBRegressor(**params))
        ])

        pipe.fit(X_train, y_train)
        preds = pipe.predict(X_val)
        rmse_scores.append(np.sqrt(mean_squared_error(y_val, preds)))

    return np.mean(rmse_scores)

# ======================================================
# === H√ÄM MAIN ƒê√É ƒê∆Ø·ª¢C VI·∫æT L·∫†I HO√ÄN TO√ÄN ===
# ======================================================
def main():
    """Main pipeline: Ch·∫°y 4 l·∫ßn, 1 l·∫ßn cho m·ªói target."""
    
    # 1. Initialize ClearML (Step 5)
    task = Task.init(
        project_name=config.CLEARML_PROJECT_NAME,
        task_name=config.CLEARML_TASK_NAME,
        tags=["Optuna", "XGBoost", "Multi-Target", "RollingOnly"]
    )
    
    # 1. T·∫£i d·ªØ li·ªáu (cho Optuna)
    X_tune_full, y_tune_dict_full = load_features_for_tuning_multi(
        config.TARGET_FORECAST_COLS
    )

    # Dictionary ƒë·ªÉ l∆∞u c√°c params t·ªët nh·∫•t
    all_best_params = {}

    # === B·ªåC TRONG V√íNG L·∫∂P ===
    for target_name in config.TARGET_FORECAST_COLS:
        print(f"\nüöÄüöÄüöÄ B·∫Øt ƒë·∫ßu quy tr√¨nh cho: {target_name} üöÄüöÄüöÄ")
        
        y_tune = y_tune_dict_full[target_name]
        
        # === CƒÇN CH·ªàNH (ALIGN) END (Cho Optuna) ===
        # Quan tr·ªçng: X√≥a c√°c h√†ng NaN ·ªü cu·ªëi (do shift) C·ª¶A TARGET N√ÄY
        valid_indices_tune = y_tune.dropna().index
        X_tune_aligned = X_tune_full.loc[valid_indices_tune]
        y_tune_aligned = y_tune.loc[valid_indices_tune]
        
        print(f"Aligning data for {target_name}: Dropped {len(y_tune) - len(valid_indices_tune)} NaN rows from end.")
        
        # 3. Run Optuna (Cho target n√†y)
        print(f"üöÄ Starting Optuna tuning for {target_name}...")
        study = optuna.create_study(direction="minimize")
        study.optimize(
            lambda trial: xgb_objective(trial, X_tune_aligned, y_tune_aligned), 
            n_trials=config.OPTUNA_TRIALS
        )
        
        best_params = study.best_params
        all_best_params[target_name] = best_params
        print(f"üèÜ Best Params found for {target_name}: {best_params}")
        
        # Log to ClearML
        task.connect(best_params, name=f'Best Hyperparameters ({target_name})')
        task.get_logger().report_scalar(f"best_rmse ({target_name})", "RMSE", value=study.best_value, iteration=0)

        # 4. T·∫°o Production Pipeline (Cho target n√†y)
        print(f"üõ†Ô∏è Creating final production pipeline for {target_name}...")
        production_pipeline = Pipeline([
            ('feature_engineering', create_feature_pipeline()),
            ('scaler', RobustScaler()),
            ('model', XGBRegressor(**best_params, random_state=42, n_jobs=-1))
        ])

        # 5. Retrain (Cho target n√†y)
        print(f"üîÑ Retraining pipeline on (Train + Val) for {target_name}...")
        train_df = pd.read_csv(os.path.join(config.PROCESSED_DATA_DIR, "data_train.csv"))
        val_df = pd.read_csv(os.path.join(config.PROCESSED_DATA_DIR, "data_val.csv"))
        all_train_data = pd.concat([train_df, val_df], ignore_index=True)
        all_train_data = all_train_data.sort_values("datetime").reset_index(drop=True)
        
        # T√°ch X v√† y (d√πng ƒë√∫ng target_name)
        y_train_full = all_train_data[target_name]
        
        # X_train_full l√† T·∫§T C·∫¢, nh∆∞ng ph·∫£i drop c√°c c·ªôt target kh√°c
        # v√† c·ªôt 'temp' g·ªëc
        cols_to_drop_prod = config.TARGET_FORECAST_COLS + [config.TARGET_COL]
        X_train_full = all_train_data.drop(columns=cols_to_drop_prod, errors='ignore')
        
        # CƒÉn ch·ªânh (Align) END cho Production
        valid_indices_prod = y_train_full.dropna().index
        X_train_full_aligned = X_train_full.loc[valid_indices_prod]
        y_train_full_aligned = y_train_full.loc[valid_indices_prod]

        production_pipeline.fit(X_train_full_aligned, y_train_full_aligned)

        # ======================================================
        # 5B. T√çNH V√Ä L∆ØU TRAIN METRICS (ƒê·ªÇ CHECK OVERFITTING)
        # ======================================================
        print(f"üìä Calculating performance on the Training Set for {target_name}...")
        
        # D·ª± ƒëo√°n tr√™n X_train_full_aligned
        y_train_pred = production_pipeline.predict(X_train_full_aligned)
        
        # y_train_full_aligned l√† ƒë√°p √°n
        y_train_actual = y_train_full_aligned
        
        # CƒÉn ch·ªânh (Align) START (Do pipeline t·ª± dropna)
        if len(y_train_pred) < len(y_train_actual):
            rows_dropped_at_start_prod = len(y_train_actual) - len(y_train_pred)
            print(f"Aligning Train predictions: Dropping first {rows_dropped_at_start_prod} rows from actuals.")
            y_train_actual_aligned = y_train_actual.iloc[rows_dropped_at_start_prod:]
        else:
            y_train_actual_aligned = y_train_actual

        train_metrics = {
            "RMSE": np.sqrt(mean_squared_error(y_train_actual_aligned, y_train_pred)),
            "MAE": mean_absolute_error(y_train_actual_aligned, y_train_pred),
            "R2": r2_score(y_train_actual_aligned, y_train_pred)
        }
        
        print("\n--- Training Set Performance ---")
        print(f"   Train MAE ({target_name}): {train_metrics['MAE']:.4f}")
        print(f"   Train R2 ({target_name}) : {train_metrics['R2']:.4f}")
        print("----------------------------------\n")
        
        # L∆∞u file metrics
        metrics_path = os.path.join(config.OUTPUT_DIR, f"train_metrics_{target_name}.yaml")
        with open(metrics_path, "w") as f:
            yaml.dump(train_metrics, f, sort_keys=False)
        print(f"üßæ Training metrics saved to: {metrics_path}")
        # ======================================================

        # 6. L∆∞u Model (Cho target n√†y)
        model_name = f"{target_name}_pipeline.pkl"
        model_path = os.path.join(config.MODEL_DIR, model_name)
        joblib.dump(production_pipeline, model_path)
        print(f"‚úÖ Production pipeline saved to: {model_path}")
    
        # ======================================================
        # 7. SAVE TO ONNX FORMAT (STEP 9) (Cho target n√†y)
        # ======================================================
        print(f"üõ†Ô∏è Creating ONNX components for {target_name}...")
        
        scaler = RobustScaler() 
        scaler.fit(X_tune_aligned) # D√πng X_tune ƒë√£ cƒÉn ch·ªânh cho target n√†y
        
        X_train_scaled = scaler.transform(X_tune_aligned)

        model_xgb = XGBRegressor(**best_params, random_state=42, n_jobs=-1)
        model_xgb.fit(X_train_scaled, y_tune_aligned)

        # L∆ØU 2 FILE RI√äNG BI·ªÜT (v·ªõi t√™n target)
        scaler_name = f"scaler_{target_name}.pkl"
        model_json_name = f"model_{target_name}.json"
        
        scaler_path = os.path.join(config.MODEL_DIR, scaler_name)
        joblib.dump(scaler, scaler_path)
        print(f"‚úÖ ONNX Scaler saved to: {scaler_path}")
        
        model_json_path = os.path.join(config.MODEL_DIR, model_json_name)
        model_xgb.save_model(model_json_path)
        print(f"‚úÖ ONNX XGBoost Model saved to: {model_json_path}")
        # ======================================================

    # Save best_params.yaml
    params_path = os.path.join(config.MODEL_DIR, "all_best_params.yaml")
    with open(params_path, "w") as f:
        yaml.dump(all_best_params, f)
    
    task.close()
    print("\nüéâüéâüéâ Completed Training. üéâüéâüéâ")

if __name__ == "__main__":
    main()