# train.py
import os
import pandas as pd
import numpy as np
import joblib
import yaml
import optuna
# XÃ³a import Pruning Ä‘á»ƒ trÃ¡nh lá»—i
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error
from xgboost import XGBRegressor
# from clearml import Task # <-- Comment/XÃ³a náº¿u khÃ´ng dÃ¹ng

# Import from other files
import config
from feature_engineering import create_feature_pipeline

# ======================================================
# HÃ€M NÃ€Y ÄÃƒ ÄÆ¯á»¢C Sá»¬A Láº I HOÃ€N TOÃ€N Äá»‚ FIX Lá»–I DATA ALIGNMENT
# ======================================================
def load_features_for_tuning(target_col):
    """
    Táº£i features (X) tá»« feature_data/ vÃ  target (y) tá»« processed_data/
    Ä‘á»ƒ Ä‘áº£m báº£o khÃ´ng cÃ³ data leakage vÃ  dá»¯ liá»‡u Ä‘Æ°á»£c Ä‘á»“ng bá»™.
    """
    print("ðŸ” Loading aligned data for tuning (X from features, y from processed)...")
    
    # 1. Táº£i FEATURES (X) (ÄÃ£ Ä‘Æ°á»£c táº¡o vÃ  dropna)
    train_feat_X_path = os.path.join(config.FEATURE_DIR, "feature_train.csv")
    val_feat_X_path = os.path.join(config.FEATURE_DIR, "feature_val.csv")
    
    if not os.path.exists(train_feat_X_path) or not os.path.exists(val_feat_X_path):
        raise FileNotFoundError(
            "Feature files not found. Please run feature_engineering.py first."
        )
        
    train_feat_X = pd.read_csv(train_feat_X_path)
    val_feat_X = pd.read_csv(val_feat_X_path)
    X_tune = pd.concat([train_feat_X, val_feat_X], ignore_index=True)

    # 2. Táº£i dá»¯ liá»‡u PROCESSED (Äá»ƒ láº¥y y)
    train_proc_path = os.path.join(config.PROCESSED_DATA_DIR, "data_train.csv")
    val_proc_path = os.path.join(config.PROCESSED_DATA_DIR, "data_val.csv")
    
    if not os.path.exists(train_proc_path) or not os.path.exists(val_proc_path):
        raise FileNotFoundError(
            "Processed data files not found. Please run data_processing.py first."
        )

    train_proc = pd.read_csv(train_proc_path)
    val_proc = pd.read_csv(val_proc_path)
    
    # 3. CÄ‚N CHá»ˆNH (ALIGN) y Vá»šI X
    # feature_engineering.py Ä‘Ã£ "dropna()" cÃ¡c hÃ ng Ä‘áº§u tiÃªn cá»§a train_feat_X
    
    original_train_len = len(train_proc)
    new_train_len = len(train_feat_X)
    rows_dropped_at_start = original_train_len - new_train_len
    
    if rows_dropped_at_start < 0:
        raise ValueError("Feature train set is larger than processed train set. Check logic.")
        
    print(f"Aligning data: {rows_dropped_at_start} rows were dropped from train set by feature_engineering (due to rolling windows).")

    # Láº¥y y (target) tá»« cÃ¡c file processed, Bá»Ž ÄI cÃ¡c hÃ ng Ä‘áº§u tiÃªn
    y_train = train_proc[target_col].iloc[rows_dropped_at_start:]
    y_val = val_proc[target_col] # Táº­p val khÃ´ng bá»‹ dropna

    y_tune = pd.concat([y_train, y_val], ignore_index=True)

    # 4. Kiá»ƒm tra láº§n cuá»‘i
    if len(X_tune) != len(y_tune):
        raise ValueError(
            f"Data misalignment: X_tune has {len(X_tune)} rows, "
            f"but y_tune has {len(y_tune)} rows. "
        )
        
    obj_cols = X_tune.select_dtypes(include=['object']).columns
    if not obj_cols.empty:
        print(f"âš ï¸ Dropping object columns from X_tune: {list(obj_cols)}")
        X_tune = X_tune.drop(columns=obj_cols)

    return X_tune, y_tune
# ======================================================

# ======================================================
# HÃ€M NÃ€Y ÄÃƒ Sá»¬A Láº I (Báº£n Ä‘Æ¡n giáº£n) Äá»‚ TRÃNH Lá»–I PHIÃŠN Báº¢N
# ======================================================
def xgb_objective(trial, X, y):
    """Objective function for Optuna (Báº£n Ä‘Æ¡n giáº£n, khÃ´ng Pruning)."""
    tscv = TimeSeriesSplit(n_splits=config.CV_SPLITS)
    rmse_scores = []

    params = {
        'n_estimators': trial.suggest_int("n_estimators", 100, 1000),
        'max_depth': trial.suggest_int("max_depth", 3, 10),
        'learning_rate': trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
        'subsample': trial.suggest_float("subsample", 0.6, 1.0),
        'colsample_bytree': trial.suggest_float("colsample_bytree", 0.6, 1.0),
        'gamma': trial.suggest_float("gamma", 0.0, 5.0),
        'min_child_weight': trial.suggest_int("min_child_weight", 1, 10),
        'random_state': 42,
        'n_jobs': -1
    }

    for train_idx, val_idx in tscv.split(X):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

        pipe = Pipeline([
            ("scaler", RobustScaler()),
            ("xgb", XGBRegressor(**params))
        ])

        pipe.fit(X_train, y_train)
        preds = pipe.predict(X_val)
        rmse_scores.append(np.sqrt(mean_squared_error(y_val, preds)))

    return np.mean(rmse_scores)
# ======================================================

def main():
    """Main pipeline: Tune -> Create Final Pipeline -> Retrain -> Save."""
    
    # 1. Initialize ClearML (Step 5)
    # task = Task.init(...) # <-- Táº¯t náº¿u khÃ´ng dÃ¹ng
    
    # 2. Load data (for tuning only)
    X_tune, y_tune = load_features_for_tuning(target_col=config.TARGET_COL)

    # 3. Run Optuna
    print(f"ðŸš€ Starting Optuna tuning ({config.OPTUNA_TRIALS} trials)...")
    study = optuna.create_study(direction="minimize")
    study.optimize(lambda trial: xgb_objective(trial, X_tune, y_tune), n_trials=config.OPTUNA_TRIALS)
    
    best_params = study.best_params
    print(f"ðŸ† Best Params found: {best_params}")
    
    # Log best params to ClearML
    # task.connect(best_params, name='Best Hyperparameters') # <-- Táº¯t náº¿u khÃ´ng dÃ¹ng
    # task.get_logger().report_scalar(...) # <-- Táº¯t náº¿u khÃ´ng dÃ¹ng

    # 4. CREATE FINAL PRODUCTION PIPELINE
    print("ðŸ› ï¸ Creating final production pipeline...")
    production_pipeline = Pipeline([
        ('feature_engineering', create_feature_pipeline()),
        ('scaler', RobustScaler()),
        ('model', XGBRegressor(**best_params, random_state=42, n_jobs=-1))
    ])

    # 5. RETRAIN ON FULL (TRAIN + VAL) DATASET
    print("ðŸ”„ Retraining pipeline on (Train + Val)...")
    train_df = pd.read_csv(os.path.join(config.PROCESSED_DATA_DIR, "data_train.csv"))
    val_df = pd.read_csv(os.path.join(config.PROCESSED_DATA_DIR, "data_val.csv"))
    
    all_train_data = pd.concat([train_df, val_df], ignore_index=True)
    all_train_data["datetime"] = pd.to_datetime(all_train_data["datetime"])
    all_train_data = all_train_data.sort_values("datetime").reset_index(drop=True)

    X_train_full = all_train_data.drop(columns=[config.TARGET_COL], errors='ignore')
    y_train_full = all_train_data[config.TARGET_COL]

    production_pipeline.fit(X_train_full, y_train_full)

    # 6. SAVE PIPELINE
    model_path = os.path.join(config.MODEL_DIR, config.MODEL_NAME)
    joblib.dump(production_pipeline, model_path)
    print(f"âœ… Production pipeline saved to: {model_path}")
    
    # ======================================================
    # 7. SAVE TO ONNX FORMAT (STEP 9) - ÄÃƒ Sá»¬A Láº I
    # ======================================================
    print("ðŸ› ï¸ Creating ONNX-convertible components (Scaler + Model)...")

    scaler = RobustScaler() 
    X_train_full_feat, y_train_full_feat = load_features_for_tuning(config.TARGET_COL)
    scaler.fit(X_train_full_feat)
    
    X_train_scaled = scaler.transform(X_train_full_feat)

    model_xgb = XGBRegressor(**best_params, random_state=42, n_jobs=-1)
    model_xgb.fit(X_train_scaled, y_train_full_feat)

    # 4. LÆ¯U 2 FILE RIÃŠNG BIá»†T
    scaler_path = os.path.join(config.MODEL_DIR, "scaler_for_onnx.pkl")
    joblib.dump(scaler, scaler_path)
    print(f"âœ… ONNX Scaler saved to: {scaler_path}")
    
    model_json_path = os.path.join(config.MODEL_DIR, "model_for_onnx.json")
    model_xgb.save_model(model_json_path)
    print(f"âœ… ONNX XGBoost Model saved to: {model_json_path}")
    # ======================================================

    # Save best_params.yaml
    params_path = os.path.join(config.MODEL_DIR, "best_params.yaml")
    with open(params_path, "w") as f:
        yaml.dump(best_params, f)
    
    # task.close() # <-- Táº¯t náº¿u khÃ´ng dÃ¹ng
    print("ðŸŽ‰ Training complete!")

if __name__ == "__main__":
    main()