# inference.py
import os
import pandas as pd
import numpy as np
import joblib
import yaml
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error
import config

def load_production_model():
    """Táº£i pipeline sáº£n pháº©m (features + scaler + model)."""
    model_path = os.path.join(config.MODEL_DIR, config.MODEL_NAME)
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"âŒ Model not found at {model_path}. Please run train.py first.")
    
    model = joblib.load(model_path)
    print(f"âœ… Production pipeline loaded from: {model_path}")
    return model

def load_test_data():
    """Táº£i dá»¯ liá»‡u test (processed, trÆ°á»›c khi táº¡o feature)."""
    test_path = os.path.join(config.PROCESSED_DATA_DIR, "data_test.csv")
    if not os.path.exists(test_path):
        raise FileNotFoundError(f"âŒ data_test.csv not found at {test_path}. Please run data_processing.py first.")
    
    df = pd.read_csv(test_path)
    df["datetime"] = pd.to_datetime(df["datetime"])
    df = df.sort_values("datetime").reset_index(drop=True)
    
    print(f"âœ… Test data loaded: {df.shape}")
    
    # TÃ¡ch X_test vÃ  y_test
    # y_test chÃ­nh lÃ  cá»™t 'temp'
    # X_test lÃ  táº¥t cáº£ cÃ¡c cá»™t cÃ²n láº¡i (dá»¯ liá»‡u thÃ´)
    X_test = df.drop(columns=[config.TARGET_COL], errors='ignore')
    y_test = df[config.TARGET_COL]
    
    return X_test, y_test, df

def evaluate_on_test(model, X_test, y_test):
    """
    ÄÃ¡nh giÃ¡ model trÃªn táº­p test (1-step-ahead forecast).
    Step 5: DÃ¹ng cÃ¡c metrics RMSE, MAPE, R2
    """
    print("âš™ï¸ Predicting on test set...")
    # Pipeline sáº½ tá»± Ä‘á»™ng cháº¡y:
    # 1. feature_engineering (FFillImputer -> TimeFeatures -> LagRolling -> DropRaw)
    # 2. scaler (RobustScaler)
    # 3. model (XGBRegressor)
    y_pred = model.predict(X_test)

    # Xá»­ lÃ½ cÃ¡c giÃ¡ trá»‹ NaN cÃ³ thá»ƒ Ä‘Æ°á»£c táº¡o ra á»Ÿ Ä‘áº§u (do rolling)
    # ChÃºng ta cáº§n cÄƒn chá»‰nh y_test vÃ  y_pred
    
    # TÃ¬m sá»‘ hÃ ng NaN á»Ÿ Ä‘áº§u y_pred (náº¿u cÃ³)
    # (Pipeline cá»§a chÃºng ta Ä‘Ã£ xá»­ lÃ½ .dropna() bÃªn trong)
    # NhÆ°ng X_test gá»‘c cÃ³ thá»ƒ dÃ i hÆ¡n y_pred
    
    if len(y_pred) < len(y_test):
        print(f"Aligning predictions: Dropping first {len(y_test) - len(y_pred)} rows from y_test to match rolling window NaNs.")
        # Bá» Ä‘i cÃ¡c hÃ ng Ä‘áº§u cá»§a y_test, tÆ°Æ¡ng á»©ng vá»›i cÃ¡c hÃ ng NaN Ä‘Ã£ bá»‹ drop
        y_test = y_test.iloc[len(y_test) - len(y_pred):]
    

    metrics = {
        "RMSE": np.sqrt(mean_squared_error(y_test, y_pred)),
        "MAE": mean_absolute_error(y_test, y_pred),
        "R2": r2_score(y_test, y_pred),
        "MAPE": mean_absolute_percentage_error(y_test, y_pred)
    }

    print("\nğŸ“Š Test Set Performance (1-step-ahead):")
    for k, v in metrics.items():
        print(f"   {k:<6}: {v:.4f}")
        
    return y_pred, y_test, metrics # Tráº£ vá» y_test Ä‘Ã£ Ä‘Æ°á»£c cÄƒn chá»‰nh

def save_results(df_test, y_test_aligned, y_pred, metrics, output_dir):
    """LÆ°u káº¿t quáº£ dá»± Ä‘oÃ¡n vÃ  metrics."""
    
    # Chá»‰ láº¥y cÃ¡c hÃ ng cá»§a df_test tÆ°Æ¡ng á»©ng vá»›i y_test Ä‘Ã£ cÄƒn chá»‰nh
    result_df = df_test.iloc[len(df_test) - len(y_test_aligned):].copy()
    
    result_df["predicted_temp"] = y_pred

    pred_path = os.path.join(output_dir, "test_predictions.csv")
    metrics_path = os.path.join(output_dir, "test_metrics.yaml")

    result_df.to_csv(pred_path, index=False)
    with open(metrics_path, "w") as f:
        yaml.dump(metrics, f, sort_keys=False)

    print(f"\nğŸ’¾ Predictions saved to: {pred_path}")
    print(f"ğŸ§¾ Metrics saved to: {metrics_path}")
    return result_df

def visualize_predictions(df_results, output_dir):
    """Váº½ biá»ƒu Ä‘á»“ so sÃ¡nh thá»±c táº¿ vs dá»± Ä‘oÃ¡n."""
    
    plt.figure(figsize=(15, 6))
    plt.plot(df_results['datetime'], df_results[config.TARGET_COL], label="Actual", marker='.', linestyle='-')
    plt.plot(df_results['datetime'], df_results['predicted_temp'], label="Predicted", marker='x', linestyle='--')
    plt.xlabel("Date")
    plt.ylabel("Temperature (Â°C)")
    plt.title("Test Set Performance (Actual vs. Predicted)")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    
    line_path = os.path.join(output_dir, "test_predictions_line_plot.png")
    plt.savefig(line_path)
    print(f"ğŸ“ˆ Line plot saved to: {line_path}")

def main():
    model = load_production_model()
    X_test, y_test, df_test = load_test_data()
    y_pred, y_test_aligned, metrics = evaluate_on_test(model, X_test, y_test)
    df_results = save_results(df_test, y_test_aligned, y_pred, metrics, config.OUTPUT_DIR)
    visualize_predictions(df_results, config.OUTPUT_DIR)
    print("\nğŸ‰ Test set evaluation complete.")

if __name__ == "__main__":
    main()