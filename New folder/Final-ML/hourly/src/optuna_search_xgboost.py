# optuna_search_xgboost.py
import os
import pandas as pd
import numpy as np
import yaml
import optuna
import xgboost as xgb # ‚¨ÖÔ∏è THAY ƒê·ªîI
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import mean_squared_error
from clearml import Task
import config
from feature_engineering import create_feature_pipeline
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# =============================================================================
# CUSTOM PURGED TIME SERIES SPLIT (ANTI-LEAKAGE)
# =============================================================================
class PurgedTimeSeriesSplit:
    # (Code c·ªßa class PurgedTimeSeriesSplit gi·ªØ nguy√™n)
    def __init__(self, n_splits=5, gap=0):
        self.n_splits = n_splits
        self.gap = gap
    def split(self, X, y=None, groups=None):
        n_samples = len(X)
        indices = np.arange(n_samples)
        test_size = n_samples // (self.n_splits + 1)
        for i in range(self.n_splits):
            train_end = (i + 1) * test_size
            val_start = train_end + self.gap
            val_end = val_start + test_size
            if val_end > n_samples:
                break
            train_indices = indices[:train_end]
            val_indices = indices[val_start:val_end]
            yield train_indices, val_indices
    def get_n_splits(self, X=None, y=None, groups=None):
        return self.n_splits

# =============================================================================
# ALIGNMENT FUNCTION (S·ª¨A L·ªñI DROPNA)
# =============================================================================
def align_data_for_tuning(X_raw, y_raw, pipeline, scaler, fit_transform=False):
    # (Code h√†m n√†y gi·ªØ nguy√™n)
    if fit_transform:
        X_feat = pipeline.fit_transform(X_raw)
        X_scaled = scaler.fit_transform(X_feat)
    else:
        X_feat = pipeline.transform(X_raw)
        X_scaled = scaler.transform(X_feat)
    y_aligned = y_raw.copy()
    y_aligned.index = X_feat.index 
    y_df = pd.DataFrame(y_aligned)
    X_df = pd.DataFrame(X_scaled, index=X_feat.index, columns=X_feat.columns) 
    combined = pd.concat([y_df, X_df], axis=1)
    combined_clean = combined.dropna(subset=[y_aligned.name]) 
    y_final = combined_clean[y_aligned.name]
    X_final = combined_clean.drop(columns=[y_aligned.name])
    return X_final, y_final

# =============================================================================
# DATA LOADING (FIXED - TH√äM B∆Ø·ªöC DROPNA QUAN TR·ªåNG)
# =============================================================================
def load_data_for_tuning(target_name):
    # (Code h√†m n√†y gi·ªØ nguy√™n)
    print(f"üîç Loading RAW data (Train + Val COMBINED) for {target_name}...")
    train_df = pd.read_csv(os.path.join(config.PROCESSED_DATA_DIR, "data_train.csv"))
    val_df = pd.read_csv(os.path.join(config.PROCESSED_DATA_DIR, "data_val.csv"))
    all_train_data = pd.concat([train_df, val_df], ignore_index=True)
    if 'datetime' not in all_train_data.columns:
         raise KeyError("‚ùå Kh√¥ng t√¨m th·∫•y c·ªôt 'datetime' trong file CSV.")
    print(f"   ...S·ª≠ d·ª•ng c·ªôt 'datetime' l√†m c·ªôt th·ªùi gian")
    all_train_data['datetime'] = pd.to_datetime(all_train_data['datetime'])
    all_train_data = all_train_data.set_index('datetime', drop=False)
    all_train_data = all_train_data.sort_index()
    rows_before = len(all_train_data)
    all_train_data = all_train_data.dropna(subset=[target_name])
    rows_after = len(all_train_data)
    if rows_before > rows_after:
        print(f"   ‚ö†Ô∏è ƒê√£ x√≥a {rows_before - rows_after} h√†ng c√≥ NaN trong c·ªôt target.")
    y_train_full_raw = all_train_data[target_name]
    X_train_full_raw = all_train_data.copy()
    print(f"‚úÖ Combined RAW data shapes: X={X_train_full_raw.shape}, y={y_train_full_raw.shape}")
    print(f"üìÖ Date range: {X_train_full_raw.index.min()} ‚Üí {X_train_full_raw.index.max()}")
    return X_train_full_raw, y_train_full_raw

# =============================================================================
# ‚¨ÖÔ∏è THAY ƒê·ªîI 1: H√ÄM OBJECTIVE CHO XGBOOST
# =============================================================================
def xgboost_objective(trial, X_all_train_raw, y_all_train_raw):
    """
    Objective function cho XGBoost (v·ªõi Early Stopping)
    """
    # ‚ùóÔ∏è ƒê·∫£m b·∫£o b·∫°n ƒë√£ ƒë·ªãnh nghƒ©a 'XGBOOST_PARAM_RANGES' trong config.py
    ranges = config.XGBOOST_PARAM_RANGES 
    
    # 1. Suggest hyperparameters
    # ‚ùóÔ∏è B·ªè "num_leaves" (c·ªßa LGBM), thay b·∫±ng c√°c param c·ªßa XGBoost
    params = {
        'learning_rate': trial.suggest_float('learning_rate', *ranges['learning_rate'], log=True),
        'max_depth': trial.suggest_int('max_depth', *ranges['max_depth']),
        'subsample': trial.suggest_float('subsample', *ranges['subsample']),
        'colsample_bytree': trial.suggest_float('colsample_bytree', *ranges['colsample_bytree']),
        'reg_alpha': trial.suggest_float('reg_alpha', *ranges['reg_alpha'], log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', *ranges['reg_lambda'], log=True),
        
        # (T√πy ch·ªçn, th√™m n·∫øu b·∫°n c√≥ 'min_child_weight' trong config)
        # 'min_child_weight': trial.suggest_int('min_child_weight', *ranges['min_child_weight']),
        
        # --- Tham s·ªë c·ªë ƒë·ªãnh cho XGBoost ---
        'objective': 'reg:squarederror',
        'booster': 'gbtree',
        'tree_method': 'hist', # D√πng 'hist' cho nhanh, ho·∫∑c 'gpu_hist' n·∫øu c√≥ GPU
        'seed': 42,
        'n_jobs': -1,
        'verbosity': 0, # T·∫Øt log c·ªßa XGBoost
        'n_estimators': 2000 # ‚úÖ S·ªë l·ªõn c·ªë ƒë·ªãnh cho early stopping
    }

    tscv = PurgedTimeSeriesSplit(
        n_splits=config.CV_N_SPLITS, 
        gap=config.CV_GAP_ROWS
    )
    fold_scores = []
    best_iterations = [] # ‚¨ÖÔ∏è M·ªöI: Theo d√µi s·ªë v√≤ng l·∫∑p t·ªët nh·∫•t
    
    for fold_num, (train_idx, val_idx) in enumerate(tscv.split(X_all_train_raw)):
        X_train_fold_raw = X_all_train_raw.iloc[train_idx]
        y_train_fold_raw = y_all_train_raw.iloc[train_idx]
        X_val_fold_raw = X_all_train_raw.iloc[val_idx]
        y_val_fold_raw = y_all_train_raw.iloc[val_idx]
        
        # (Ph·∫ßn in log Fold 1 gi·ªØ nguy√™n)
        if fold_num == 0:
            train_dates_col = X_train_fold_raw['datetime'] 
            val_dates_col = X_val_fold_raw['datetime']
            gap_duration = (val_dates_col.min() - train_dates_col.max())
            print(f"   ‚úÖ Fold 1 verified:")
            print(f"       Train: {train_dates_col.min()} ‚Üí {train_dates_col.max()}")
            print(f"       Gap:   {gap_duration} (approx {config.CV_GAP_DAYS} days)")
            print(f"       Val:   {val_dates_col.min()} ‚Üí {val_dates_col.max()}")

        feature_pipeline_fold = create_feature_pipeline()
        scaler_fold = RobustScaler()
        X_train_fold, y_train_fold = align_data_for_tuning(
            X_train_fold_raw, y_train_fold_raw, 
            feature_pipeline_fold, scaler_fold, 
            fit_transform=True
        )
        X_val_fold, y_val_fold = align_data_for_tuning(
            X_val_fold_raw, y_val_fold_raw, 
            feature_pipeline_fold, scaler_fold, 
            fit_transform=False
        )

        # ‚¨ÖÔ∏è THAY ƒê·ªîI: Kh·ªüi t·∫°o XGBRegressor
        model = xgb.XGBRegressor(**params)
        
        if X_train_fold.empty or y_train_fold.empty:
            print(f"   ‚ö†Ô∏è Fold {fold_num+1} r·ªóng. B·ªè qua.")
            continue

        # ‚¨ÖÔ∏è THAY ƒê·ªîI: C√∫ ph√°p .fit() c·ªßa XGBoost cho early stopping
        model.fit(
            X_train_fold, y_train_fold,
            eval_set=[(X_val_fold, y_val_fold)],
            early_stopping_rounds=100, # ‚¨ÖÔ∏è Tham s·ªë c·ªßa XGBoost
            verbose=False # ‚¨ÖÔ∏è T·∫Øt log khi fit
        )
        
        # ‚¨ÖÔ∏è THAY ƒê·ªîI: L·∫•y s·ªë v√≤ng l·∫∑p t·ªët nh·∫•t (kh√¥ng c√≥ d·∫•u _ )
        best_iteration = model.best_iteration
        if best_iteration is None or best_iteration <= 0:
            best_iteration = params['n_estimators'] # Fallback
        best_iterations.append(best_iteration)
            
        # ‚¨ÖÔ∏è THAY ƒê·ªîI: predict() c·ªßa XGBoost t·ª± ƒë·ªông d√πng best_iteration
        # kh√¥ng c·∫ßn tham s·ªë num_iteration
        y_val_pred = model.predict(X_val_fold)
        
        val_rmse = np.sqrt(mean_squared_error(y_val_fold, y_val_pred))
        fold_scores.append(val_rmse)
    
    final_rmse = np.mean(fold_scores)
    avg_best_iteration = int(np.mean(best_iterations)) # ‚¨ÖÔ∏è M·ªöI: T√≠nh s·ªë v√≤ng l·∫∑p TB
    
    trial.set_user_attr("val_rmse", float(final_rmse))
    # ‚¨ÖÔ∏è M·ªöI: L∆∞u l·∫°i s·ªë v√≤ng l·∫∑p TB ƒë·ªÉ d√πng khi train
    trial.set_user_attr("avg_best_iteration", avg_best_iteration) 
    
    return final_rmse

# =============================================================================
# ‚¨ÖÔ∏è THAY ƒê·ªîI 2: H√ÄM MAIN
# =============================================================================
def run_optuna_search_xgboost(): # ‚¨ÖÔ∏è ƒê·ªïi t√™n h√†m
    task = Task.init(
        project_name=config.CLEARML_PROJECT_NAME,
        task_name="Optuna XGBoost (Hourly)", # ‚¨ÖÔ∏è ƒê·ªïi t√™n
        tags=["Optuna", "XGBoost", "Multi-Horizon", "Purged-CV", "Hourly"] # ‚¨ÖÔ∏è ƒê·ªïi Tag
    )
    
    all_best_params = {}
    all_best_scores = {}
    all_best_details = {}

    for target_name in config.TARGET_FORECAST_COLS:
        print("\n" + "="*80)
        print(f"üéØ TUNING XGBOOST FOR: {target_name}") # ‚¨ÖÔ∏è ƒê·ªïi t√™n
        print("="*80)
    
        X_all_train_raw, y_all_train_raw = load_data_for_tuning(target_name)
        
        print(f"üîç Starting Optuna search (XGBoost)...") # ‚¨ÖÔ∏è ƒê·ªïi t√™n
        print(f"   Strategy: {config.CV_N_SPLITS}-Fold Purged TimeSeriesSplit (Gap={config.CV_GAP_ROWS} rows/hours)")
        print(f"   Trials:   {config.OPTUNA_TRIALS}")
        print(f"   ‚ö†Ô∏è   This will take time...\n")
        
        study = optuna.create_study(direction="minimize")
        study.optimize(
            lambda trial: xgboost_objective(trial, X_all_train_raw, y_all_train_raw), # ‚¨ÖÔ∏è G·ªçi h√†m objective m·ªõi
            n_trials=config.OPTUNA_TRIALS,
            show_progress_bar=True
        )
        
        best_trial = study.best_trial
        best_params = best_trial.params
        val_rmse = best_trial.user_attrs.get("val_rmse", 0)
        # ‚¨ÖÔ∏è M·ªöI: L·∫•y s·ªë v√≤ng l·∫∑p t·ªët nh·∫•t t·ª´ trial
        avg_best_iter = best_trial.user_attrs.get("avg_best_iteration", 0) 

        # ‚¨ÖÔ∏è M·ªöI: GHI ƒê√à 'n_estimators' b·∫±ng s·ªë v√≤ng l·∫∑p t√¨m ƒë∆∞·ª£c
        # ƒê√¢y l√† tham s·ªë quan tr·ªçng nh·∫•t ƒë·ªÉ train model cu·ªëi c√πng
        best_params['n_estimators'] = avg_best_iter

        all_best_params[target_name] = best_params
        all_best_scores[target_name] = float(val_rmse)
        all_best_details[target_name] = {
            "val_rmse": float(val_rmse), 
            "avg_best_iteration": avg_best_iter, # ‚¨ÖÔ∏è M·ªöI: L∆∞u l·∫°i
            "n_folds": config.CV_N_SPLITS, 
            "gap_rows": config.CV_GAP_ROWS
        }

        print(f"\nüèÜ BEST XGBOOST RESULTS FOR {target_name}:") # ‚¨ÖÔ∏è ƒê·ªïi t√™n
        print(f"   Avg Val RMSE: {val_rmse:.4f} (across {config.CV_N_SPLITS} folds)")
        # ‚¨ÖÔ∏è S·ª¨A L·ªñI LOGGING: In ra s·ªë v√≤ng l·∫∑p T√åM ƒê∆Ø·ª¢C (kh√¥ng ph·∫£i s·ªë 2000 c·ªë ƒë·ªãnh)
        print(f"   Best n_estimators (avg): {avg_best_iter}")
        print(f"   Best learning_rate: {best_params.get('learning_rate'):.6f}")

    output = {
        "best_params": all_best_params,
        "best_scores": all_best_scores,
        "details": all_best_details,
        "config": {
            "n_trials": config.OPTUNA_TRIALS,
            "cv_strategy": "PurgedTimeSeriesSplit",
            "n_splits": config.CV_N_SPLITS,
            "gap_rows": config.CV_GAP_ROWS,
            # ‚ùóÔ∏è Nh·ªõ ƒë·ªïi t√™n n√†y trong config.py
            "search_space": config.XGBOOST_PARAM_RANGES, # ‚¨ÖÔ∏è ƒê·ªïi t√™n
            "leakage_safe": True
        }
    }
    
    # ‚¨ÖÔ∏è THAY ƒê·ªîI 3: T√™n file output
    # ‚ùóÔ∏è Nh·ªõ th√™m OPTUNA_RESULTS_XGBOOST_YAML v√†o config.py
    output_path = os.path.join(config.MODEL_DIR, config.OPTUNA_RESULTS_XGBOOST_YAML)
    with open(output_path, "w") as f:
        yaml.dump(output, f, sort_keys=False, default_flow_style=False)
    
    print("\n" + "="*80)
    print("‚úÖ OPTUNA XGBOOST SEARCH COMPLETE (LEAKAGE-FREE)") # ‚¨ÖÔ∏è ƒê·ªïi t√™n
    print("="*80)
    print(f"üìÅ Best params saved to: {output_path}")
    print(f"\nüìä Summary of Best RMSE:")
    for target, score in all_best_scores.items():
        print(f"   {target}: {score:.4f}")
    
    # ‚¨ÖÔ∏è THAY ƒê·ªîI 4: B∆∞·ªõc ti·∫øp theo
    print(f"\nüöÄ NEXT STEP: Run 'python train_xgboost.py' to train final models") # ‚¨ÖÔ∏è ƒê·ªïi t√™n
    print("="*80)
    
    task.close()

if __name__ == "__main__":
    run_optuna_search_xgboost() # ‚¨ÖÔ∏è ƒê·ªïi t√™n h√†m