import os
import time
import joblib
import numpy as np
import onnxruntime as ort
import config  # T·ªáp config.py c·ªßa b·∫°n

# --- TH√äM V√ÄO ---
# Import c√°c th∆∞ vi·ªán ƒë·ªÉ l∆∞u k·∫øt qu·∫£
import pandas as pd
import yaml
import warnings

# B·ªè qua c√°c c·∫£nh b√°o kh√¥ng c·∫ßn thi·∫øt
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
# --- K·∫æT TH√öC TH√äM V√ÄO ---


# ======================================================
# 1. C√ÄI ƒê·∫∂T
# ======================================================
MODEL_DIR = config.MODEL_DIR

PIPELINE_FILENAME = 'onnx_convertible_pipeline.pkl'
PIPELINE_PATH = os.path.join(MODEL_DIR, PIPELINE_FILENAME)

# Ch·ªçn m·ªôt m√¥ h√¨nh ƒë·ªÉ benchmark
TARGET_DAY = config.TARGET_FORECAST_COLS[0] # V√≠ d·ª•: 'target_T+1'
MODEL_NAME = f"{TARGET_DAY}_{config.MODEL_NAME}" # V√≠ d·ª•: 'target_T+1_model_daily'

PKL_PATH = os.path.join(MODEL_DIR, MODEL_NAME) # <-- ƒê√£ th√™m .pkl
ONNX_PATH = os.path.join(MODEL_DIR, f"{MODEL_NAME}.onnx")

# C·∫•u h√¨nh benchmark
N_SAMPLES = 1000  # S·ªë l∆∞·ª£ng m·∫´u trong 1 l√¥
N_ITERATIONS = 100 # Ch·∫°y bao nhi√™u l·∫ßn ƒë·ªÉ l·∫•y trung b√¨nh


def get_num_features_from_pipeline(pipeline_path):
    """
    T·∫£i pipeline ti·ªÅn x·ª≠ l√Ω v√† ƒë·∫øm s·ªë l∆∞·ª£ng features ƒë·∫ßu ra
    t·ª´ b∆∞·ªõc 'preprocess_columns'.
    """
    if not os.path.exists(pipeline_path):
        print(f"‚ùå Error: Pipeline not found at {pipeline_path}")
        print(f"Please run train_linear.py first to create '{PIPELINE_FILENAME}'")
        return None
    
    try:
        full_preprocessing_pipeline = joblib.load(pipeline_path)
        feature_pipeline = full_preprocessing_pipeline.named_steps['feature_engineering']
        preprocessor_step = feature_pipeline.named_steps['preprocess_columns']
        num_features = len(preprocessor_step.final_cols)
        
        print(f"‚úÖ Loaded pipeline. Detected {num_features} output features from ColumnPreprocessor.")
        return num_features
        
    except Exception as e:
        print(f"‚ùå Error loading pipeline or getting feature count: {e}")
        return None

# ======================================================
# 2. T·∫†O D·ªÆ LI·ªÜU ƒê·∫¶U V√ÄO GI·∫¢
# ======================================================
print("üöÄ Starting Inference Benchmark...")
num_features = get_num_features_from_pipeline(PIPELINE_PATH)
if num_features is None:
    exit()

print(f"Creating dummy data: ({N_SAMPLES}, {num_features}) features.")
dummy_data = np.random.rand(N_SAMPLES, num_features).astype(np.float32)

# ======================================================
# 3. T·∫¢I C√ÅC MODEL
# ======================================================

print("Loading models...")
# 3a. T·∫£i model Sklearn (.pkl)
try:
    model_sklearn = joblib.load(PKL_PATH)
    print(f"‚úÖ Successfully loaded {PKL_PATH}")
except Exception as e:
    print(f"‚ùå Error loading {PKL_PATH}: {e}")
    exit()

# 3b. T·∫£i model ONNX (cho CPU)
try:
    sess_onnx_cpu = ort.InferenceSession(
        ONNX_PATH, 
        providers=['CPUExecutionProvider']
    )
    input_name = sess_onnx_cpu.get_inputs()[0].name
    print(f"‚úÖ Successfully loaded {ONNX_PATH} for CPU.")
except Exception as e:
    print(f"‚ùå Error loading {ONNX_PATH} for CPU: {e}")
    exit()

# 3c. T·∫£i model ONNX (cho GPU)
sess_onnx_gpu = None
try:
    sess_onnx_gpu = ort.InferenceSession(
        ONNX_PATH, 
        providers=['CUDAExecutionProvider']
    )
    print(f"‚úÖ Successfully loaded {ONNX_PATH} for GPU (CUDA).")
except Exception as e:
    print(f"‚ö†Ô∏è Could not load model for GPU (CUDA): {e}")
    print("   Make sure you have 'onnxruntime-gpu' installed and have NVIDIA/CUDA drivers.")

# ======================================================
# 4. CH·∫†Y BENCHMARK
# ======================================================
print("\n" + "="*50)
print(f"Running benchmark with {N_SAMPLES} samples, repeating {N_ITERATIONS} times...")
print("="*50)

# 4a. Benchmark Sklearn (CPU)
start_time = time.perf_counter()
for _ in range(N_ITERATIONS):
    _ = model_sklearn.predict(dummy_data)
end_time = time.perf_counter()
sklearn_time = (end_time - start_time) / N_ITERATIONS
print(f"‚è±Ô∏è Sklearn (CPU) : {sklearn_time * 1000:.6f} ms / batch")

# 4b. Benchmark ONNX (CPU)
start_time = time.perf_counter()
for _ in range(N_ITERATIONS):
    _ = sess_onnx_cpu.run(None, {input_name: dummy_data})
end_time = time.perf_counter()
onnx_cpu_time = (end_time - start_time) / N_ITERATIONS
print(f"‚è±Ô∏è ONNX (CPU)    : {onnx_cpu_time * 1000:.6f} ms / batch")

# 4c. Benchmark ONNX (GPU)
onnx_gpu_time = None # Kh·ªüi t·∫°o
if sess_onnx_gpu:
    start_time = time.perf_counter()
    for _ in range(N_ITERATIONS):
        _ = sess_onnx_gpu.run(None, {input_name: dummy_data})
    end_time = time.perf_counter()
    onnx_gpu_time = (end_time - start_time) / N_ITERATIONS
    print(f"‚è±Ô∏è ONNX (GPU)    : {onnx_gpu_time * 1000:.6f} ms / batch")

# ======================================================
# 5. HI·ªÇN TH·ªä V√Ä L∆ØU K·∫æT QU·∫¢ (PHONG C√ÅCH M·ªöI)
# ======================================================
print("\n" + "="*70)
print("üèÜ FINAL INFERENCE BENCHMARK RESULTS üèÜ")
print("="*70)

# 1. X√¢y d·ª±ng danh s√°ch k·∫øt qu·∫£
results = []

results.append({
    "Method": "Sklearn (CPU)",
    "Time_ms_per_batch": sklearn_time * 1000,
    "Speedup_vs_Sklearn": 1.0  # Baseline
})

results.append({
    "Method": "ONNX (CPU)",
    "Time_ms_per_batch": onnx_cpu_time * 1000,
    "Speedup_vs_Sklearn": sklearn_time / onnx_cpu_time
})

if onnx_gpu_time:
    results.append({
        "Method": "ONNX (GPU)",
        "Time_ms_per_batch": onnx_gpu_time * 1000,
        "Speedup_vs_Sklearn": sklearn_time / onnx_gpu_time
    })

# 2. Chuy·ªÉn sang DataFrame ƒë·ªÉ in
results_df = pd.DataFrame(results).sort_values(by="Time_ms_per_batch")
print(results_df.to_string(index=False, float_format="%.4f"))

# 3. L∆∞u file YAML
# B·∫†N C·∫¶N TH√äM BI·∫æN N√ÄY V√ÄO config.py
INFERENCE_BENCHMARK_YAML = "inference_benchmark.yaml" 
# Ho·∫∑c, n·∫øu b·∫°n ƒë√£ ƒë·ªãnh nghƒ©a n√≥ trong config, h√£y d√πng:
# INFERENCE_BENCHMARK_YAML = config.INFERENCE_BENCHMARK_YAML

output_path = os.path.join(config.OUTPUT_DIR, INFERENCE_BENCHMARK_YAML)

# Chuy·ªÉn ƒë·ªïi sang dict
results_dict = results_df.to_dict('records')

try:
    with open(output_path, "w") as f:
        yaml.dump(results_dict, f, sort_keys=False)
    print(f"\nüíæ Inference benchmark results saved to: {output_path}")
except Exception as e:
    print(f"\n‚ùå Error saving inference benchmark results: {e}")